# LLM Benchmark Suite

A comprehensive benchmarking system for evaluating Large Language Models on coding tasks, featuring a Streamlit UI and support for multiple benchmark datasets.

## ğŸ¯ Features

- **Multiple Benchmarks**: HumanEval, MBPP, SWE-bench, BigCodeBench
- **Streamlit UI**: Interactive web interface for running benchmarks and viewing results
- **Model Support**: OpenAI, Anthropic Claude, Google Gemini, Ollama (local models)
- **Detailed Metrics**: Pass rates, latency, token usage, and cost tracking
- **Comprehensive Reports**: JSON and text summaries with task-level details

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Run Streamlit UI

```bash
streamlit run app.py
```

### 3. Or Use Command Line

```bash
python runner.py --model ollama:qwen2.5-coder:7b --benchmark human_eval --limit 10
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ app.py                      # Streamlit UI
â”œâ”€â”€ runner.py                   # CLI benchmark runner
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ run.sh                      # Quick run script
â”œâ”€â”€ setup_docker.sh            # Docker setup for SWE-bench
â”‚
â”œâ”€â”€ benchmarks/                 # Benchmark implementations
â”‚   â”œâ”€â”€ __init__.py            # Registry
â”‚   â”œâ”€â”€ base.py                # Base classes
â”‚   â”œâ”€â”€ humaneval.py           # HumanEval benchmark
â”‚   â”œâ”€â”€ mbpp.py                # MBPP benchmark
â”‚   â”œâ”€â”€ bigcodebench.py        # BigCodeBench benchmark
â”‚   â”œâ”€â”€ swe_bench.py           # SWE-bench demo
â”‚   â”œâ”€â”€ swe_bench_full.py      # SWE-bench full dataset
â”‚   â””â”€â”€ swe_bench_official.py  # SWE-bench official evaluation
â”‚
â”œâ”€â”€ models/                     # Model adapters
â”‚   â”œâ”€â”€ __init__.py            # Model registry
â”‚   â”œâ”€â”€ base.py                # Base adapter
â”‚   â”œâ”€â”€ openai_adapter.py      # OpenAI models
â”‚   â”œâ”€â”€ claude_adapter.py      # Anthropic Claude
â”‚   â”œâ”€â”€ gemini_adapter.py      # Google Gemini
â”‚   â””â”€â”€ ollama_adapter.py      # Ollama (local)
â”‚
â”œâ”€â”€ scripts/                    # Helper scripts
â”‚   â””â”€â”€ run_single.py          # Run single task
â”‚
â””â”€â”€ Documentation/              # User guides
    â”œâ”€â”€ QUICKSTART.md
    â”œâ”€â”€ INSTALLATION.md
    â”œâ”€â”€ DATASETS.md
    â”œâ”€â”€ BIGCODEBENCH.md
    â””â”€â”€ ...
```

## ğŸ® Available Benchmarks

### HumanEval

- 164 Python programming problems
- Function implementation tasks
- Fast evaluation (~5-10 minutes for full set)

### MBPP (Mostly Basic Python Problems)

- 500 entry-level Python problems
- Beginner to intermediate difficulty
- Good for testing basic coding abilities

### BigCodeBench

- 1,140 practical coding tasks
- Multi-library function calls
- Two modes: `instruct` (NL) and `complete` (docstrings)

### SWE-bench

- Real-world GitHub issues
- Repository-level code changes
- Requires Docker for evaluation

## ğŸ¤– Supported Models

### Cloud APIs

- **OpenAI**: GPT-4, GPT-3.5, etc.
- **Anthropic**: Claude 3 (Opus, Sonnet, Haiku)
- **Google**: Gemini Pro, Gemini Flash

### Local (via Ollama)

- Qwen2.5-Coder
- DeepSeek Coder
- Code Llama
- And any other Ollama model

## ğŸ“Š Metrics Tracked

- **Pass Rate**: Percentage of tasks solved correctly
- **Latency**: Average time per task
- **Token Usage**: Input, output, and total tokens
- **Cost**: Estimated API costs (for cloud models)
- **Failure Analysis**: Categorized error types

## ğŸ”§ Configuration

### Environment Variables

For cloud models, set API keys:

```bash
export OPENAI_API_KEY="your-key"
export ANTHROPIC_API_KEY="your-key"
export GOOGLE_API_KEY="your-key"
```

### Ollama Setup

For local models:

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a model
ollama pull qwen2.5-coder:7b
```

## ğŸ“– Documentation

See the `Documentation/` folder for detailed guides:

- [Installation Guide](Documentation/INSTALLATION.md)
- [Quick Start Guide](Documentation/QUICKSTART.md)
- [Dataset Information](Documentation/DATASETS.md)
- [BigCodeBench Guide](Documentation/BIGCODEBENCH.md)
- [SWE-bench Setup](Documentation/SWE_BENCH_SETUP.md)

## ğŸ§ª Running Tests

The test files are excluded from the repository. For development:

```bash
# Example: Test HumanEval
python -c "from benchmarks import registry; b = registry.create('human_eval', limit=3); print(list(b.load_tasks()))"
```

## ğŸ“ Output

Results are saved to `reports/` directory:

- `benchmark_<name>_<timestamp>.json` - Full results
- `benchmark_<name>_<timestamp>_summary.txt` - Human-readable summary

## ğŸ¤ Contributing

This is a benchmarking tool. To add new benchmarks:

1. Create a new file in `benchmarks/`
2. Extend the `Benchmark` base class
3. Register in `benchmarks/__init__.py`

## ğŸ“„ License

See LICENSE file for details.

## ğŸ™ Acknowledgments

- **HumanEval**: OpenAI
- **MBPP**: Google
- **SWE-bench**: Princeton NLP
- **BigCodeBench**: BigCode Project
